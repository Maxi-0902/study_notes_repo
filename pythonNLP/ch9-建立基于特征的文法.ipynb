{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第9章 建立基于特征的文法\n",
    "\n",
    "自然语言具有范围广泛的**文法结构**，用第8章中所描述的简单的方法很难处理的如此广泛的文法结构。为了获得更大的灵活性，我们改变我们对待文法类别如 `S`、`NP` 和 `V` 的方式。 我们将这些原子标签分解为类似字典的结构，以便可以提取一系列的值作为特征。 \n",
    "\n",
    "本章的目的是要回答下列问题： \n",
    "1. 我们怎样用特征扩展上下文无关文法框架，以获得更细粒度的对文法类别和产生式的控制？\n",
    "2. 特征结构的主要形式化属性是什么，我们如何使用它们来计算？ \n",
    "3. 我们现在用基于特征的文法能捕捉到什么语言模式和文法结构？ \n",
    "\n",
    "一路上，我们将介绍更多的英语句法主题，包括：约定、子类别和无限制依赖成分等现象。\n",
    "\n",
    "### 9.1 文法特征\n",
    "\n",
    "在第6 章，我们描述了如何通过检测文本的特征建立分类器。那些特征可能非常简单，如提取一个单词的最后一个字母，或者更复杂一点儿，如分类器自己预测的词性标签。在本章中，我们将探讨在建立基于规则的文法中特征的作用。对比特征提取，记录已经自动检测到的特征，我们现在要声明词和短语的特征。我们以一个很简单的例子开始，使用字典存储特征和它们的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "kim = {'CAT': 'NP', 'ORTH': 'Kim', 'REF': 'k'}\n",
    "chase = {'CAT': 'V', 'ORTH': 'chased', 'REL': 'chase'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对象`kim` 和`chase` 有几个共同的特征，**`CAT`（文法类别）**和 **`ORTH`（正字法，即拼写）**。\n",
    "此外，每一个还有更面向语义的特征：`kim['REF']`意在给出 `kim` 的指示物，而 `chase['REL']`给出 `chase` 表示的关系。在基于规则的文法上下文中，这样的特征和特征值对被称为**特征结构**，我们将很快看到它们的替代符号。\n",
    "\n",
    "特征结构包含各种有关文法实体的信息。这些信息不需要详尽无遗，我们可能要进一步增加属性。例如：一个动词，知道它扮演的“语义角色”往往很有用。对于`chase`，主语扮演“`agent`（施事）”的角色，而宾语扮演“`patient`（受事）”角色。让我们添加这些信息，使 用`'sbj'`（主语）和`'obj'`（宾语）作为占位符，它会被填充，当动词和它的文法参数结合时："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chase['AGT'] = 'sbj'\n",
    "chase['PAT'] = 'obj'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们现在处理句子：`Kim chased Lee`，我们要“绑定”动词的**施事角色**和**主语**，**受事角色和宾语**。我们可以通过链接到相关的 `NP` 的 `REF` 特征做到这个。在下面的例子中， 我们做一个简单的假设：在动词直接左侧和右侧的 `NP` 分别是主语和宾语。我们还在例子结尾为 `Lee` 添加了一个特征结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORTH  => chased\n",
      "REL   => chase\n",
      "AGT   => k\n",
      "PAT   => l\n"
     ]
    }
   ],
   "source": [
    "sent = \"Kim chased Lee\"\n",
    "tokens = sent.split()\n",
    "lee = {'CAT': 'NP', 'ORTH': 'Lee', 'REF': 'l'}\n",
    "def lex2fs(word):\n",
    "    for fs in [kim, lee, chase]:\n",
    "        if fs['ORTH'] == word:\n",
    "            return fs\n",
    "subj, verb, obj = lex2fs(tokens[0]), lex2fs(tokens[1]), lex2fs(tokens[2])\n",
    "verb['AGT'] = subj['REF'] # # agent of 'chase' is Kim\n",
    "verb['PAT'] = obj['REF'] # patient of 'chase' is Lee\n",
    "for k in ['ORTH', 'REL', 'AGT', 'PAT']:\n",
    "    print(\"%-5s => %s\" % (k, verb[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征结构是非常强大的，但我们操纵它们的方式是极其特别的。我们本章接下来的任务是，显示上下文无关文法和分析如何能扩展到合适的特征结构，使我们可以一种更通用的和有原则的方式建立像这样的分析。我们将通过查看句法协议的现象作为开始；我们将展示如何使用特征典雅的表示协议约束，并在一个简单的文法中说明它们的用法。\n",
    "\n",
    "由于**特征结构是表示任何形式的信息的通用的数据结构**，我们将从更形式化的视点简要地看着它们，并演示 NLTK 提供的特征结构的支持。在本章的最后一部分，我们将表明，特征的额外表现力开辟了一个用于描述语言结构的复杂性的广泛的可能性。\n",
    "\n",
    "\n",
    "#### 句法协议\n",
    "\n",
    "在英语中，名词通常被标记为单数或复数。要么两个都是单数要么都是复数。主语和谓词间也存在类似的约束。这里我们可以看到，动词的形态属性与主语名词短语的句法属性一起变化。这种一起变化被称为**协议**（`agreement`）。如果我们进一步看**英语动词协议**，我们将看到动词的现在时态通常有两种屈折形式：一为**第三人称单数**，另一个为**人称和数量的所有其他组合**。动词与它的主语在人称和数量上保持一致。\n",
    "\n",
    "#### 使用属性和约束\n",
    "\n",
    "我们说过非正式的语言类别具有属性，例如：名词具有复数的属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "# ###################\n",
      "# Grammar Productions\n",
      "# ###################\n",
      "# S expansion productions\n",
      "S -> NP[NUM=?n] VP[NUM=?n]\n",
      "# NP expansion productions\n",
      "NP[NUM=?n] -> N[NUM=?n] \n",
      "NP[NUM=?n] -> PropN[NUM=?n] \n",
      "NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]\n",
      "NP[NUM=pl] -> N[NUM=pl] \n",
      "# VP expansion productions\n",
      "VP[TENSE=?t, NUM=?n] -> IV[TENSE=?t, NUM=?n]\n",
      "VP[TENSE=?t, NUM=?n] -> TV[TENSE=?t, NUM=?n] NP\n",
      "# ###################\n",
      "# Lexical Productions\n",
      "# ###################\n",
      "Det[NUM=sg] -> 'this' | 'every'\n",
      "Det[NUM=pl] -> 'these' | 'all'\n",
      "Det -> 'the' | 'some' | 'several'\n",
      "PropN[NUM=sg]-> 'Kim' | 'Jody'\n",
      "N[NUM=sg] -> 'dog' | 'girl' | 'car' | 'child'\n",
      "N[NUM=pl] -> 'dogs' | 'girls' | 'cars' | 'children' \n",
      "IV[TENSE=pres,  NUM=sg] -> 'disappears' | 'walks'\n",
      "TV[TENSE=pres, NUM=sg] -> 'sees' | 'likes'\n",
      "IV[TENSE=pres,  NUM=pl] -> 'disappear' | 'walk'\n",
      "TV[TENSE=pres, NUM=pl] -> 'see' | 'like'\n",
      "IV[TENSE=past] -> 'disappeared' | 'walked'\n",
      "TV[TENSE=past] -> 'saw' | 'liked'\n"
     ]
    }
   ],
   "source": [
    "# 例9-1. 基于特征的文法的例子\n",
    "import nltk\n",
    "nltk.data.show_cfg('grammars/book_grammars/feat0.fcfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意一个句法类别可以有多个特征，例如：`V[TENSE=pres, NUM=pl]`。在一般情况下，我们喜欢多少特征就可以添加多少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|.Kim .like.chil.|\n",
      "Leaf Init Rule:\n",
      "|[----]    .    .| [0:1] 'Kim'\n",
      "|.    [----]    .| [1:2] 'likes'\n",
      "|.    .    [----]| [2:3] 'children'\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[----]    .    .| [0:1] PropN[NUM='sg'] -> 'Kim' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[----]    .    .| [0:1] NP[NUM='sg'] -> PropN[NUM='sg'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[---->    .    .| [0:1] S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'sg'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    [----]    .| [1:2] TV[NUM='sg', TENSE='pres'] -> 'likes' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    [---->    .| [1:2] VP[NUM=?n, TENSE=?t] -> TV[NUM=?n, TENSE=?t] * NP[] {?n: 'sg', ?t: 'pres'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [----]| [2:3] N[NUM='pl'] -> 'children' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [----]| [2:3] NP[NUM='pl'] -> N[NUM='pl'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [---->| [2:3] S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'pl'}\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|.    [---------]| [1:3] VP[NUM='sg', TENSE='pres'] -> TV[NUM='sg', TENSE='pres'] NP[] *\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|[==============]| [0:3] S[] -> NP[NUM='sg'] VP[NUM='sg'] *\n"
     ]
    }
   ],
   "source": [
    "# 例9-2. 跟踪基于特征的图表分析器\n",
    "tokens = 'Kim likes children'.split()\n",
    "from nltk import load_parser\n",
    "cp = load_parser('grammars/book_grammars/feat0.fcfg', trace=2)\n",
    "trees = cp.parse(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[NUM='sg'] (PropN[NUM='sg'] Kim))\n",
      "  (VP[NUM='sg', TENSE='pres']\n",
      "    (TV[NUM='sg', TENSE='pres'] likes)\n",
      "    (NP[NUM='pl'] (N[NUM='pl'] children))))\n"
     ]
    }
   ],
   "source": [
    "# 最后，我们可以检查生成的分析树\n",
    "for tree in trees: print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 术语\n",
    "\n",
    "到目前为止，我们只看到像 `sg`和`pl` 这样的特征值。这些简单的值通常被称为**原子**。原子值的一种特殊情况是布尔值，例如：我们可能要用布尔特征`AUX`区分助动词。\n",
    "\n",
    "\n",
    "### 8.2 处理特征结构\n",
    "\n",
    "在本节中，我们将展示如何构建特征结构，并在 NLTK 中操作。我们还将讨论统一的基本操作，这使我们能够结合两个不同的特征结构中的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ NUM   = 'sg'   ]\n",
      "[ TENSE = 'past' ]\n"
     ]
    }
   ],
   "source": [
    "fs1 = nltk.FeatStruct(TENSE='past', NUM='sg')\n",
    "print(fs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个特征结构实际上只是一种字典，所以我们可以平常的方式通过索引访问它的值。我们可以用我们熟悉的方式指定值给特征："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fem\n"
     ]
    }
   ],
   "source": [
    "fs1 = nltk.FeatStruct(PER=3, NUM='pl', GND='fem')\n",
    "print(fs1['GND'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs1['CASE'] = 'acc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       [ CASE = 'acc' ] ]\n",
      "[ AGR = [ GND  = 'fem' ] ]\n",
      "[       [ NUM  = 'pl'  ] ]\n",
      "[       [ PER  = 3     ] ]\n",
      "[                        ]\n",
      "[ POS = 'N'              ]\n"
     ]
    }
   ],
   "source": [
    "fs2 = nltk.FeatStruct(POS='N', AGR=fs1)\n",
    "print(fs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ CASE = 'acc' ]\n",
      "[ GND  = 'fem' ]\n",
      "[ NUM  = 'pl'  ]\n",
      "[ PER  = 3     ]\n"
     ]
    }
   ],
   "source": [
    "print(fs2['AGR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(fs2['AGR']['PER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       [ GND = 'fem' ] ]\n",
      "[ AGR = [ NUM = 'pl'  ] ]\n",
      "[       [ PER = 3     ] ]\n",
      "[                       ]\n",
      "[ POS = 'N'             ]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.FeatStruct(\"[POS='N', AGR=[PER=3, NUM='pl', GND='fem']]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ age   = 33               ]\n",
      "[ name  = 'Lee'            ]\n",
      "[ telno = '01 27 86 42 96' ]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.FeatStruct(name='Lee', telno='01 27 86 42 96', age=33))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将特征结构作为图来查看往往是有用的，更具体的，作为有向无环图（`directed acyclic graphs，DAGs`）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ADDRESS = (1) [ NUMBER = 74           ] ]\n",
      "[               [ STREET = 'rue Pascal' ] ]\n",
      "[                                         ]\n",
      "[ NAME    = 'Lee'                         ]\n",
      "[                                         ]\n",
      "[ SPOUSE  = [ ADDRESS -> (1)  ]           ]\n",
      "[           [ NAME    = 'Kim' ]           ]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.FeatStruct(\"\"\"[NAME='Lee', ADDRESS=(1)[NUMBER=74, STREET='rue Pascal'], \n",
    "SPOUSE=[NAME='Kim', ADDRESS->(1)]]\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 包含和统一\n",
    "认为特征结构提供一些对象的部分信息是很正常的，在这个意义上，我们可以根据它们通用的程度给特征结构排序。这个顺序被称为包含（`subsumption`）；一个更一般的特征结构包含（ `subsumes`）一个较少一般的。\n",
    "\n",
    "合并两个特征结构的信息被称为统一，由方法 `unify()`支持。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ CITY   = 'Paris'      ]\n",
      "[ NUMBER = 74           ]\n",
      "[ STREET = 'rue Pascal' ]\n"
     ]
    }
   ],
   "source": [
    "fs1 = nltk.FeatStruct(NUMBER=74, STREET='rue Pascal')\n",
    "fs2 = nltk.FeatStruct(CITY='Paris')\n",
    "print(fs1.unify(fs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ CITY   = 'Paris'      ]\n",
      "[ NUMBER = 74           ]\n",
      "[ STREET = 'rue Pascal' ]\n"
     ]
    }
   ],
   "source": [
    "print(fs2.unify(fs1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 扩展基于特征的文法\n",
    "\n",
    "#### 子类别\n",
    "#### 核心词回顾\n",
    "#### 助动词与倒装\n",
    "#### 无限制依赖成分\n",
    "#### 德语中的格和性别\n",
    "\n",
    "与英语相比，德语的协议具有相对丰富的形态。例如：在德语中定冠词根据格、性别和 数量变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "# ###################\n",
      "# Grammar Productions\n",
      "# ###################\n",
      "S[-INV] -> NP VP\n",
      "S[-INV]/?x -> NP VP/?x\n",
      "S[-INV] -> NP S/NP\n",
      "S[-INV] -> Adv[+NEG] S[+INV]\n",
      "S[+INV] -> V[+AUX] NP VP\n",
      "S[+INV]/?x -> V[+AUX] NP VP/?x\n",
      "SBar -> Comp S[-INV]\n",
      "SBar/?x -> Comp S[-INV]/?x\n",
      "VP -> V[SUBCAT=intrans, -AUX]\n",
      "VP -> V[SUBCAT=trans, -AUX] NP\n",
      "VP/?x -> V[SUBCAT=trans, -AUX] NP/?x\n",
      "VP -> V[SUBCAT=clause, -AUX] SBar\n",
      "VP/?x -> V[SUBCAT=clause, -AUX] SBar/?x\n",
      "VP -> V[+AUX] VP\n",
      "VP/?x -> V[+AUX] VP/?x\n",
      "# ###################\n",
      "# Lexical Productions\n",
      "# ###################\n",
      "V[SUBCAT=intrans, -AUX] -> 'walk' | 'sing'\n",
      "V[SUBCAT=trans, -AUX] -> 'see' | 'like'\n",
      "V[SUBCAT=clause, -AUX] -> 'say' | 'claim'\n",
      "V[+AUX] -> 'do' | 'can'\n",
      "NP[-WH] -> 'you' | 'cats'\n",
      "NP[+WH] -> 'who'\n",
      "Adv[+NEG] -> 'rarely' | 'never'\n",
      "NP/NP ->\n",
      "Comp -> 'that'\n"
     ]
    }
   ],
   "source": [
    "# 例9-3. 具有倒装从句和长距离依赖的产生式的文法，使用斜线类别\n",
    "nltk.data.show_cfg('grammars/book_grammars/feat1.fcfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[-INV]\n",
      "  (NP[+WH] who)\n",
      "  (S[+INV]/NP[]\n",
      "    (V[+AUX] do)\n",
      "    (NP[-WH] you)\n",
      "    (VP[]/NP[]\n",
      "      (V[-AUX, SUBCAT='clause'] claim)\n",
      "      (SBar[]/NP[]\n",
      "        (Comp[] that)\n",
      "        (S[-INV]/NP[]\n",
      "          (NP[-WH] you)\n",
      "          (VP[]/NP[] (V[-AUX, SUBCAT='trans'] like) (NP[]/NP[] )))))))\n"
     ]
    }
   ],
   "source": [
    "tokens = 'who do you claim that you like'.split()\n",
    "from nltk import load_parser\n",
    "cp = load_parser('grammars/book_grammars/feat1.fcfg')\n",
    "for tree in cp.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "# Grammar Productions\n",
      "S -> NP[CASE=nom, AGR=?a] VP[AGR=?a]\n",
      "NP[CASE=?c, AGR=?a] -> PRO[CASE=?c, AGR=?a]\n",
      "NP[CASE=?c, AGR=?a] -> Det[CASE=?c, AGR=?a] N[CASE=?c, AGR=?a]\n",
      "VP[AGR=?a] -> IV[AGR=?a]\n",
      "VP[AGR=?a] -> TV[OBJCASE=?c, AGR=?a] NP[CASE=?c]\n",
      "# Lexical Productions\n",
      "# Singular determiners\n",
      "# masc\n",
      "Det[CASE=nom, AGR=[GND=masc,PER=3,NUM=sg]] -> 'der' \n",
      "Det[CASE=dat, AGR=[GND=masc,PER=3,NUM=sg]] -> 'dem'\n",
      "Det[CASE=acc, AGR=[GND=masc,PER=3,NUM=sg]] -> 'den'\n",
      "# fem\n",
      "Det[CASE=nom, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die' \n",
      "Det[CASE=dat, AGR=[GND=fem,PER=3,NUM=sg]] -> 'der'\n",
      "Det[CASE=acc, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die' \n",
      "# Plural determiners\n",
      "Det[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'die' \n",
      "Det[CASE=dat, AGR=[PER=3,NUM=pl]] -> 'den' \n",
      "Det[CASE=acc, AGR=[PER=3,NUM=pl]] -> 'die' \n",
      "# Nouns\n",
      "N[AGR=[GND=masc,PER=3,NUM=sg]] -> 'Hund'\n",
      "N[CASE=nom, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
      "N[CASE=dat, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunden'\n",
      "N[CASE=acc, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
      "N[AGR=[GND=fem,PER=3,NUM=sg]] -> 'Katze'\n",
      "N[AGR=[GND=fem,PER=3,NUM=pl]] -> 'Katzen'\n",
      "# Pronouns\n",
      "PRO[CASE=nom, AGR=[PER=1,NUM=sg]] -> 'ich'\n",
      "PRO[CASE=acc, AGR=[PER=1,NUM=sg]] -> 'mich'\n",
      "PRO[CASE=dat, AGR=[PER=1,NUM=sg]] -> 'mir'\n",
      "PRO[CASE=nom, AGR=[PER=2,NUM=sg]] -> 'du'\n",
      "PRO[CASE=nom, AGR=[PER=3,NUM=sg]] -> 'er' | 'sie' | 'es'\n",
      "PRO[CASE=nom, AGR=[PER=1,NUM=pl]] -> 'wir'\n",
      "PRO[CASE=acc, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
      "PRO[CASE=dat, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
      "PRO[CASE=nom, AGR=[PER=2,NUM=pl]] -> 'ihr'\n",
      "PRO[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'sie'\n",
      "# Verbs\n",
      "IV[AGR=[NUM=sg,PER=1]] -> 'komme'\n",
      "IV[AGR=[NUM=sg,PER=2]] -> 'kommst'\n",
      "IV[AGR=[NUM=sg,PER=3]] -> 'kommt'\n",
      "IV[AGR=[NUM=pl, PER=1]] -> 'kommen'\n",
      "IV[AGR=[NUM=pl, PER=2]] -> 'kommt'\n",
      "IV[AGR=[NUM=pl, PER=3]] -> 'kommen'\n",
      "TV[OBJCASE=acc, AGR=[NUM=sg,PER=1]] -> 'sehe' | 'mag'\n",
      "TV[OBJCASE=acc, AGR=[NUM=sg,PER=2]] -> 'siehst' | 'magst'\n",
      "TV[OBJCASE=acc, AGR=[NUM=sg,PER=3]] -> 'sieht' | 'mag'\n",
      "TV[OBJCASE=dat, AGR=[NUM=sg,PER=1]] -> 'folge' | 'helfe'\n",
      "TV[OBJCASE=dat, AGR=[NUM=sg,PER=2]] -> 'folgst' | 'hilfst'\n",
      "TV[OBJCASE=dat, AGR=[NUM=sg,PER=3]] -> 'folgt' | 'hilft'\n",
      "TV[OBJCASE=acc, AGR=[NUM=pl,PER=1]] -> 'sehen' | 'moegen'\n",
      "TV[OBJCASE=acc, AGR=[NUM=pl,PER=2]] -> 'sieht' | 'moegt'\n",
      "TV[OBJCASE=acc, AGR=[NUM=pl,PER=3]] -> 'sehen' | 'moegen'\n",
      "TV[OBJCASE=dat, AGR=[NUM=pl,PER=1]] -> 'folgen' | 'helfen'\n",
      "TV[OBJCASE=dat, AGR=[NUM=pl,PER=2]] -> 'folgt' | 'helft'\n",
      "TV[OBJCASE=dat, AGR=[NUM=pl,PER=3]] -> 'folgen' | 'helfen'\n"
     ]
    }
   ],
   "source": [
    "# 例9-4. 基于特征的文法的例子\n",
    "nltk.data.show_cfg('grammars/book_grammars/german.fcfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[AGR=[NUM='sg', PER=1], CASE='nom']\n",
      "    (PRO[AGR=[NUM='sg', PER=1], CASE='nom'] ich))\n",
      "  (VP[AGR=[NUM='sg', PER=1]]\n",
      "    (TV[AGR=[NUM='sg', PER=1], OBJCASE='dat'] folge)\n",
      "    (NP[AGR=[GND='fem', NUM='pl', PER=3], CASE='dat']\n",
      "      (Det[AGR=[NUM='pl', PER=3], CASE='dat'] den)\n",
      "      (N[AGR=[GND='fem', NUM='pl', PER=3]] Katzen))))\n"
     ]
    }
   ],
   "source": [
    "tokens = 'ich folge den Katzen'.split()\n",
    "cp = load_parser('grammars/book_grammars/german.fcfg')\n",
    "for tree in cp.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 小结\n",
    "\n",
    "- 上下文无关文法的传统分类是原子符号。特征结构的一个重要的作用是捕捉精细的区分，否则将需要数量翻倍的原子类别。\n",
    "- 通过使用特征值上的变量，我们可以表达文法产生式中的限制，允许不同的特征规格的实现可以相互依赖。\n",
    "- 通常情况下，我们在词汇层面指定固定的特征值，限制短语中的特征值与它们的孩子中的对应值统一。\n",
    "- 特征值可以是原子的或复杂的。原子值的一个特定类别是布尔值，按照惯例用`[+/- feat]`表示。\n",
    "- 两个特征可以共享一个值（原子的或复杂的）。具有共享值的结构被称为重入。共享的值被表示为`AVM` 中的数字索引（或标记）。\n",
    "- 一个特征结构中的路径是一个特征的元组，对应从图的根开始的弧的序列上的标签。\n",
    "- 两条路径是等价的，如果它们共享一个值。\n",
    "- 包含的特征结构是偏序的。`FS0`包含 `FS1`，当 `FS0`比`FS1`更一般（较少信息）。\n",
    "- 两种结构`FS0` 和`FS1`的统一，如果成功，就是包含 `FS0`和`FS1`的合并信息的特征结构 `FS2`。\n",
    "- 如果统一在`FS` 中指定一条路径`π`，那么它也指定等效与`π`的每个路径`π'`。\n",
    "- 我们可以使用特征结构建立对大量广泛语言学现象的简洁的分析，包括动词子类别，倒装结构，无限制依赖结构和格支配。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d12a6980af1de3549060b7b451d48d445ec6b4aaeaf0b0e12a509d2182e95745"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
